{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train):\n",
    "    # Define transformation\n",
    "    transform = transforms.ToTensor()\n",
    "    \n",
    "    # Load data\n",
    "    if train :\n",
    "        train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "        train_x = train_dataset.data.reshape((-1, 28*28)).float().numpy()\n",
    "        train_y = train_dataset.targets.numpy()\n",
    "\n",
    "        return train_x, train_y\n",
    "    \n",
    "    else:\n",
    "        test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "        test_x = test_dataset.data.reshape((-1, 28*28)).float().numpy()\n",
    "        test_y = test_dataset.targets.numpy()\n",
    "        return test_x, test_y\n",
    "    \n",
    "\n",
    "def preprocess_data(x,y) :\n",
    "    x = x / 255.0\n",
    "        \n",
    "    num_classes = len(np.unique(y))\n",
    "    y = np.eye(num_classes)[y]\n",
    "\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xaiver_init(n_in, n_out):\n",
    "    return np.random.randn(n_in, n_out) * np.sqrt(2/(n_in+n_out))\n",
    "\n",
    "\n",
    "class Layer: \n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def backward(self, output_grad, learning_rate):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def update(self, weight, bias, weight_grad, bias_grad):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "class MiniBatchGradientDescent(Optimizer):\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update(self, weight, bias, weight_grad, bias_grad):\n",
    "        weight -= self.learning_rate * weight_grad\n",
    "        bias -= self.learning_rate * bias_grad\n",
    "        \n",
    "        return weight, bias\n",
    "    \n",
    "class AdamOptimizer(Optimizer):\n",
    "    def __init__(self, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m_w, self.v_w = None, None\n",
    "        self.m_b, self.v_b = None, None\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, weight, bias, weight_grad, bias_grad):\n",
    "        # Initialize moving averages with the shape of weights and biases on the first call\n",
    "        if self.m_w is None:\n",
    "            self.m_w = np.zeros_like(weight)\n",
    "            self.v_w = np.zeros_like(weight)\n",
    "            self.m_b = np.zeros_like(bias)\n",
    "            self.v_b = np.zeros_like(bias)\n",
    "\n",
    "        # Increment timestep\n",
    "        self.t += 1\n",
    "\n",
    "        # Update moving averages of gradients\n",
    "        self.m_w = self.beta1 * self.m_w + (1 - self.beta1) * weight_grad\n",
    "        self.v_w = self.beta2 * self.v_w + (1 - self.beta2) * (weight_grad ** 2)\n",
    "        m_w_hat = self.m_w / (1 - self.beta1 ** self.t)\n",
    "        v_w_hat = self.v_w / (1 - self.beta2 ** self.t)\n",
    "        weight -= self.learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "\n",
    "        # Update moving averages for bias\n",
    "        self.m_b = self.beta1 * self.m_b + (1 - self.beta1) * bias_grad\n",
    "        self.v_b = self.beta2 * self.v_b + (1 - self.beta2) * (bias_grad ** 2)\n",
    "        m_b_hat = self.m_b / (1 - self.beta1 ** self.t)\n",
    "        v_b_hat = self.v_b / (1 - self.beta2 ** self.t)\n",
    "        bias -= self.learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "        return weight, bias\n",
    "\n",
    "    \n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, n_in, n_out, initializer=xaiver_init, optimizer=None):\n",
    "        self.weight = initializer(n_in, n_out)\n",
    "        self.bias = initializer(1, n_out)\n",
    "        self.optimizer = optimizer if optimizer is not None else AdamOptimizer(0.001)\n",
    "        self.weight_grad = None\n",
    "        self.bias_grad = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.dot(input, self.weight) + self.bias\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_grad, learning_rate):\n",
    "        input_grad = np.dot(output_grad, self.weight.T)\n",
    "        self.weight_grad = np.dot(self.input.T, output_grad)\n",
    "        self.bias_grad = np.sum(output_grad, axis=0)\n",
    "        self.weight, self.bias = self.optimizer.update(self.weight, self.bias, self.weight_grad, self.bias_grad)\n",
    "        return input_grad\n",
    "\n",
    "    \n",
    "class ReLu(Layer):\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.maximum(0, input)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_grad, learning_rate):\n",
    "        return output_grad * (self.input > 0)\n",
    "    \n",
    "\n",
    "class Softmax(Layer):\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Shift inputs for numerical stability\n",
    "        exp = np.exp(input - np.max(input, axis=1, keepdims=True))\n",
    "        self.output = exp / np.sum(exp, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, y_true, learning_rate=None):\n",
    "        # Directly return the gradient for softmax + cross-entropy\n",
    "        return y_true\n",
    "\n",
    "\n",
    "\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, dropout_rate):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, input, train=True):\n",
    "        self.input = input\n",
    "        if train:\n",
    "            self.mask = (np.random.rand(*input.shape) > self.dropout_rate) / (1 - self.dropout_rate)\n",
    "            self.output = input * self.mask\n",
    "        else:\n",
    "            self.output = input\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_grad, learning_rate):\n",
    "        return np.multiply(output_grad, self.mask) / (1 - self.dropout_rate)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    # Clip predictions to avoid log(0)\n",
    "    y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)\n",
    "    return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n",
    "\n",
    "\n",
    "def softmax_cross_entropy_gradient(y_true, y_pred):\n",
    "    # Derivative of softmax + cross-entropy loss\n",
    "    return (y_pred - y_true) / y_true.shape[0]\n",
    "\n",
    "\n",
    "class BatchNormalization(Layer):\n",
    "    def __init__(self, input_size, momentum=0.9, epsilon=1e-5):\n",
    "        self.gamma = np.ones(input_size)  # Scaling parameter (gamma)\n",
    "        self.beta = np.zeros(input_size)  # Shifting parameter (beta)\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        # Running mean and variance for inference\n",
    "        self.running_mean = np.zeros(input_size)\n",
    "        self.running_var = np.ones(input_size)\n",
    "        \n",
    "    def forward(self, x, train=True):\n",
    "        \"\"\"\n",
    "        Forward pass for batch normalization.\n",
    "        \"\"\"\n",
    "        if train:\n",
    "            # Compute sample mean and variance for the batch\n",
    "            sample_mean = np.mean(x, axis=0)\n",
    "            sample_var = np.var(x, axis=0)\n",
    "            \n",
    "            # Update running mean and variance for inference\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * sample_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * sample_var\n",
    "            \n",
    "            # Normalize batch\n",
    "            self.std = np.sqrt(sample_var + self.epsilon)\n",
    "            self.x_centered = x - sample_mean\n",
    "            self.x_norm = self.x_centered / self.std\n",
    "            self.output = self.gamma * self.x_norm + self.beta\n",
    "            \n",
    "            # Cache values for backward pass\n",
    "            self.cache = (self.x_norm, self.x_centered, self.std, self.gamma)\n",
    "        else:\n",
    "            # Use running statistics to normalize during inference\n",
    "            self.x_norm = (x - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "            self.output = self.gamma * self.x_norm + self.beta\n",
    "            \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dout, learning_rate):\n",
    "        \"\"\"\n",
    "        Backward pass for batch normalization.\n",
    "        \"\"\"\n",
    "        N, D = dout.shape\n",
    "        x_norm, x_centered, std, gamma = self.cache\n",
    "        \n",
    "        # Gradients with respect to gamma and beta\n",
    "        dgamma = np.sum(dout * x_norm, axis=0)\n",
    "        dbeta = np.sum(dout, axis=0)\n",
    "        \n",
    "        # Gradient of the normalized input\n",
    "        dx_norm = dout * gamma\n",
    "        dx = (1 / N) / std * (N * dx_norm - np.sum(dx_norm, axis=0) - x_norm * np.sum(dx_norm * x_norm, axis=0))\n",
    "        \n",
    "        # Update gamma and beta using gradients\n",
    "        self.gamma -= learning_rate * dgamma\n",
    "        self.beta -= learning_rate * dbeta\n",
    "        \n",
    "        return dx\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, x, train=True):\n",
    "        for layer in self.layers:\n",
    "          \n",
    "            if isinstance(layer, (BatchNormalization, Dropout)):\n",
    "                x = layer.forward(x, train)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def backward(self, y_true, learning_rate):\n",
    "        # Initialize output gradient from the last layer\n",
    "        output_grad = self.layers[-1].backward(y_true, learning_rate)\n",
    "        \n",
    "        # Propagate backwards through all layers\n",
    "        for layer in reversed(self.layers[:-1]):  \n",
    "            output_grad = layer.backward(output_grad, learning_rate)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        # Create a copy of the network for prediction, excluding dropout layers\n",
    "        pred_nn = NeuralNetwork(self.layers)\n",
    "        pred_nn.layers = [layer for layer in self.layers if not isinstance(layer, Dropout)]\n",
    "        \n",
    "        # Forward pass through the network with dropout disabled\n",
    "        y_pred = pred_nn.forward(X, train=False)\n",
    "        \n",
    "        # Return the predicted class labels as integers\n",
    "        return np.argmax(y_pred, axis=1)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        # Get predictions in single-label format\n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        # Convert one-hot encoded true labels to single-label format if necessary\n",
    "        if y.ndim > 1:  # Check if y is one-hot encoded\n",
    "            y = np.argmax(y, axis=1)\n",
    "        \n",
    "        # Calculate accuracy and macro F1 score\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        f1 = f1_score(y, y_pred, average='macro')\n",
    "        \n",
    "        return accuracy, f1\n",
    "\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_val, y_val, epochs, learning_rate, batch_size):\n",
    "        epoch_loss, epoch_accuracy, epoch_val_loss, epoch_val_accuracy, epoch_val_f1 = [], [], [], [], []\n",
    "\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            train_loss = 0\n",
    "            for j in range(0, len(X_train), batch_size):\n",
    "                X_batch = X_train[j:j + batch_size]\n",
    "                y_batch = y_train[j:j + batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred = self.forward(X_batch, train=True)\n",
    "\n",
    "                # Compute and accumulate training loss\n",
    "                batch_loss = cross_entropy_loss(y_batch, y_pred)\n",
    "                train_loss += batch_loss\n",
    "\n",
    "                # Backward pass\n",
    "                output_gradient = y_pred - y_batch\n",
    "                self.backward(output_gradient, learning_rate)\n",
    "\n",
    "            # Calculate and record metrics for each epoch\n",
    "            avg_epoch_loss = train_loss / (len(X_train) / batch_size)\n",
    "            epoch_loss.append(avg_epoch_loss)\n",
    "            \n",
    "            val_pred = self.forward(X_val, train=False)\n",
    "            val_loss = cross_entropy_loss(y_val, val_pred)\n",
    "            epoch_val_loss.append(val_loss)\n",
    "\n",
    "            train_accuracy, _ = self.evaluate(X_train, y_train)\n",
    "            epoch_accuracy.append(train_accuracy)\n",
    "\n",
    "            val_accuracy, val_f1 = self.evaluate(X_val, y_val)\n",
    "            epoch_val_accuracy.append(val_accuracy)\n",
    "            epoch_val_f1.append(val_f1)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_epoch_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "                f\"Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        return epoch_loss, epoch_val_loss, epoch_accuracy, epoch_val_accuracy, epoch_val_f1\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def save_confusion_matrix(self, X, y, filename=\"confusion_matrix.png\"):\n",
    "        y_pred = self.predict(X)  # Predict on the input data\n",
    "\n",
    "        # Check if `y` is one-hot encoded\n",
    "        if len(y.shape) > 1 and y.shape[1] > 1:\n",
    "            y_true = np.argmax(y, axis=1)  # Convert one-hot encoded labels to integer labels\n",
    "        else:\n",
    "            y_true = y  # Already integer labels\n",
    "\n",
    "        # Calculate confusion matrix\n",
    "        all_classes = np.unique(np.concatenate([y_true, y_pred]))  # Ensure all possible labels are included\n",
    "        matrix = confusion_matrix(y_true, y_pred, labels=all_classes)\n",
    "\n",
    "        # Convert confusion matrix to a DataFrame for better visualization\n",
    "        df_cm = pd.DataFrame(matrix, index=all_classes, columns=all_classes)\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(df_cm, annot=True, cmap=\"Blues\", fmt='g')\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.savefig(filename)\n",
    "        plt.clf()\n",
    "\n",
    "        return matrix\n",
    "\n",
    "\n",
    "    def save_graphs(self, epoch_loss, epoch_val_loss, epoch_accuracy, epoch_val_accuracy, epoch_val_f1):\n",
    "        \n",
    "        # Plot and save the training and validation loss graph\n",
    "        plt.plot(epoch_loss, label=\"Train loss\")\n",
    "        plt.plot(epoch_val_loss, label=\"Validation loss\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Validation Loss\")\n",
    "        plt.savefig(\"loss_graph.png\")\n",
    "        plt.clf()\n",
    "\n",
    "        # Plot and save the training and validation accuracy graph\n",
    "        plt.plot(epoch_accuracy, label=\"Train accuracy\")\n",
    "        plt.plot(epoch_val_accuracy, label=\"Validation accuracy\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.title(\"Training and Validation Accuracy\")\n",
    "        plt.savefig(\"accuracy_graph.png\")\n",
    "        plt.clf()\n",
    "\n",
    "        # Plot and save the validation F1 score graph\n",
    "        plt.plot(epoch_val_f1, label=\"Validation F1 score\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Macro-F1\")\n",
    "        plt.title(\"Validation F1 Score\")\n",
    "        plt.savefig(\"f1_graph.png\")\n",
    "        plt.clf()\n",
    "\n",
    "    def clean_model(self):\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dense):  # Only keep weights and biases for Dense layers\n",
    "                layer.input = None\n",
    "                layer.output = None\n",
    "            elif isinstance(layer, Dropout):  # Drop intermediate variables for Dropout layers\n",
    "                layer.input = None\n",
    "                layer.output = None\n",
    "                layer.mask = None\n",
    "            elif isinstance(layer, BatchNormalization):  # Drop intermediate variables for BatchNormalization layers\n",
    "                layer.input = None\n",
    "                layer.output = None\n",
    "                layer.cache = None\n",
    "            else:  # Drop input/output for other layers\n",
    "                layer.input = None\n",
    "                layer.output = None\n",
    "\n",
    "    def save_model(self, filename=\"model.pkl\"):\n",
    "        \n",
    "        self.clean_model()  # Clean the model to reduce size\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(self, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def load_test_data():\n",
    "    # Step 1: Load the test set from the pickle file\n",
    "    with open('a2.pkl', 'rb') as file:\n",
    "        test_data = pickle.load(file)\n",
    "\n",
    "    # Step 2: Check if the dataset is a TensorDataset\n",
    "    if isinstance(test_data, torch.utils.data.dataset.TensorDataset):\n",
    "        # Extract images and labels\n",
    "        test_images, test_labels = test_data.tensors\n",
    "        test_images = test_images.numpy()  \n",
    "        test_labels = test_labels.numpy()  \n",
    "        \n",
    "    \n",
    "        print(f\"Images shape: {test_images.shape}\")\n",
    "        print(f\"Labels shape: {test_labels.shape}\")\n",
    "\n",
    "    \n",
    "        plt.figure(figsize=(10, 3))\n",
    "        for i in range(5):\n",
    "            plt.subplot(1, 5, i+1)\n",
    "            plt.imshow(test_images[i].squeeze(), cmap='gray')  \n",
    "            plt.title(f\"Label: {test_labels[i]}\")\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "  \n",
    "        num_classes = len(np.unique(test_labels))\n",
    "        print(f\"Number of possible classifications: {num_classes}\")\n",
    "\n",
    "        unique_labels, counts = np.unique(test_labels, return_counts=True)\n",
    "        print(\"\\nNumber of examples for each label:\")\n",
    "        for label, count in zip(unique_labels, counts):\n",
    "            print(f\"Label {label}: {count} examples\")\n",
    "\n",
    "        \n",
    "        test_images_reshaped = test_images.reshape(test_images.shape[0], 28 * 28)\n",
    "        \n",
    "        # Determine the range and scale to 0 to 1\n",
    "        if np.min(test_images_reshaped) >= -1 and np.max(test_images_reshaped) <= 1:\n",
    "            # Scale from -1 to 1 to 0 to 1\n",
    "            print(\"\\nData is in range -1 to 1. Scaling to 0 to 1...\")\n",
    "            test_images_scaled = (test_images_reshaped + 1) / 2\n",
    "        elif np.min(test_images_reshaped) >= 0 and np.max(test_images_reshaped) <= 255:\n",
    "            # Scale from 0 to 255 to 0 to 1\n",
    "            print(\"\\nData is in range 0 to 255. Scaling to 0 to 1...\")\n",
    "            test_images_scaled = test_images_reshaped / 255.0\n",
    "        else:\n",
    "            print(\"\\nData is in an unexpected range. Please check the dataset.\")\n",
    "            return None, None\n",
    "\n",
    "        print(\"\\nTest images reshaped and scaled successfully.\")\n",
    "        \n",
    "        return test_images_scaled, test_labels\n",
    "\n",
    "    else:\n",
    "        print(\"The dataset format is not as expected. Please check the file structure.\")\n",
    "        return None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Images shape: (3611, 1, 28, 28)\n",
      "Labels shape: (3611,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of possible classifications: 4\n",
      "\n",
      "Number of examples for each label:\n",
      "Label 0: 654 examples\n",
      "Label 1: 495 examples\n",
      "Label 5: 853 examples\n",
      "Label 8: 1609 examples\n",
      "\n",
      "Data is in range -1 to 1. Scaling to 0 to 1...\n",
      "\n",
      "Test images reshaped and scaled successfully.\n",
      "Initializing model architecture...\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:07<02:54,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - Loss: 1.0433, Val Loss: 0.5444, Train Acc: 0.7964, Val Acc: 0.7902, Val F1: 0.7756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [00:14<02:41,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25 - Loss: 0.6364, Val Loss: 0.4645, Train Acc: 0.8434, Val Acc: 0.8389, Val F1: 0.8372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [00:20<02:30,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25 - Loss: 0.5606, Val Loss: 0.4144, Train Acc: 0.8632, Val Acc: 0.8580, Val F1: 0.8556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4/25 [00:27<02:20,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25 - Loss: 0.5254, Val Loss: 0.3952, Train Acc: 0.8741, Val Acc: 0.8628, Val F1: 0.8632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [00:33<02:14,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25 - Loss: 0.4979, Val Loss: 0.4025, Train Acc: 0.8726, Val Acc: 0.8613, Val F1: 0.8628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [00:41<02:09,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25 - Loss: 0.4751, Val Loss: 0.3823, Train Acc: 0.8788, Val Acc: 0.8674, Val F1: 0.8669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [00:47<02:02,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25 - Loss: 0.4608, Val Loss: 0.3862, Train Acc: 0.8793, Val Acc: 0.8644, Val F1: 0.8652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [00:54<01:55,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25 - Loss: 0.4450, Val Loss: 0.3708, Train Acc: 0.8858, Val Acc: 0.8718, Val F1: 0.8707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9/25 [01:01<01:49,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25 - Loss: 0.4356, Val Loss: 0.3664, Train Acc: 0.8912, Val Acc: 0.8766, Val F1: 0.8751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10/25 [01:08<01:44,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25 - Loss: 0.4292, Val Loss: 0.3580, Train Acc: 0.8921, Val Acc: 0.8761, Val F1: 0.8752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [01:15<01:35,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 - Loss: 0.4240, Val Loss: 0.3597, Train Acc: 0.8938, Val Acc: 0.8779, Val F1: 0.8772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12/25 [01:21<01:28,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25 - Loss: 0.4094, Val Loss: 0.3554, Train Acc: 0.8956, Val Acc: 0.8780, Val F1: 0.8772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [01:28<01:20,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25 - Loss: 0.4099, Val Loss: 0.3552, Train Acc: 0.8995, Val Acc: 0.8823, Val F1: 0.8828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14/25 [01:37<01:21,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25 - Loss: 0.3980, Val Loss: 0.3491, Train Acc: 0.8994, Val Acc: 0.8812, Val F1: 0.8809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15/25 [01:45<01:16,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25 - Loss: 0.3932, Val Loss: 0.3541, Train Acc: 0.9004, Val Acc: 0.8804, Val F1: 0.8820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [01:53<01:10,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25 - Loss: 0.3938, Val Loss: 0.3445, Train Acc: 0.9031, Val Acc: 0.8797, Val F1: 0.8796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17/25 [02:00<01:00,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25 - Loss: 0.3870, Val Loss: 0.3406, Train Acc: 0.9053, Val Acc: 0.8852, Val F1: 0.8856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18/25 [02:07<00:50,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25 - Loss: 0.3810, Val Loss: 0.3511, Train Acc: 0.9008, Val Acc: 0.8813, Val F1: 0.8798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 19/25 [02:14<00:42,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25 - Loss: 0.3770, Val Loss: 0.3404, Train Acc: 0.9063, Val Acc: 0.8842, Val F1: 0.8848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20/25 [02:20<00:35,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25 - Loss: 0.3675, Val Loss: 0.3450, Train Acc: 0.9033, Val Acc: 0.8782, Val F1: 0.8793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21/25 [02:27<00:27,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25 - Loss: 0.3698, Val Loss: 0.3386, Train Acc: 0.9050, Val Acc: 0.8833, Val F1: 0.8819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22/25 [02:34<00:20,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25 - Loss: 0.3646, Val Loss: 0.3345, Train Acc: 0.9105, Val Acc: 0.8876, Val F1: 0.8867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23/25 [02:41<00:13,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25 - Loss: 0.3602, Val Loss: 0.3401, Train Acc: 0.9090, Val Acc: 0.8842, Val F1: 0.8848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24/25 [02:48<00:06,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25 - Loss: 0.3550, Val Loss: 0.3440, Train Acc: 0.9082, Val Acc: 0.8841, Val F1: 0.8843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [02:55<00:00,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25 - Loss: 0.3558, Val Loss: 0.3335, Train Acc: 0.9121, Val Acc: 0.8914, Val F1: 0.8914\n",
      "Saving training and validation graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on test set...\n",
      "Test Accuracy: 0.7355, Test F1 Score: 0.3591\n",
      "Saving confusion matrix...\n",
      "Saving the model...\n",
      "Process complete!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# main \n",
    "\n",
    "\n",
    "def main():\n",
    "    # Step 1: Load and Preprocess Data\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    X_train, y_train = load_data(train=True)\n",
    "    # X_test, y_test = load_data(train=False)\n",
    "    X_test, y_test = load_test_data()\n",
    "    \n",
    "    # Normalize and one-hot encode the labels\n",
    "    X_train, y_train = preprocess_data(X_train, y_train)\n",
    "    # X_test, y_test = preprocess_data(X_test, y_test)\n",
    "    \n",
    "    # Split validation set from training set\n",
    "    val_split = int(0.85 * len(X_train))\n",
    "    X_val, y_val = X_train[val_split:], y_train[val_split:]\n",
    "    X_train, y_train = X_train[:val_split], y_train[:val_split]\n",
    "    \n",
    "    # Step 2: Define Model Architecture\n",
    "    print(\"Initializing model architecture...\")\n",
    "    model = NeuralNetwork([\n",
    "    Dense(784, 256, optimizer=AdamOptimizer(learning_rate=0.003)),\n",
    "    BatchNormalization(256),\n",
    "    ReLu(),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(256, 128, optimizer=AdamOptimizer(learning_rate=0.003)),\n",
    "    BatchNormalization(128),\n",
    "    ReLu(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Dense(128, 64, optimizer=AdamOptimizer(learning_rate=0.003)),\n",
    "    BatchNormalization(64),\n",
    "    ReLu(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(64, 32, optimizer=AdamOptimizer(learning_rate=0.003)),\n",
    "    BatchNormalization(32),\n",
    "    ReLu(),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(32, 16, optimizer=AdamOptimizer(learning_rate=0.003)),\n",
    "    BatchNormalization(16),\n",
    "    ReLu(),\n",
    "    Dropout(0.1),\n",
    "\n",
    "    Dense(16, 10, optimizer=AdamOptimizer(learning_rate=0.003)),\n",
    "    Softmax()\n",
    "])\n",
    "    \n",
    "    # Step 3: Train the Model\n",
    "    print(\"Training the model...\")\n",
    "    epochs = 25\n",
    "    learning_rate = 0.003\n",
    "    batch_size = 256\n",
    "    \n",
    "    epoch_loss, epoch_val_loss, epoch_accuracy, epoch_val_accuracy, epoch_val_f1 = model.fit(\n",
    "        X_train, y_train, X_val, y_val, epochs=epochs, learning_rate=learning_rate, batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Step 4: Save Training and Validation Graphs\n",
    "    print(\"Saving training and validation graphs...\")\n",
    "    model.save_graphs(epoch_loss, epoch_val_loss, epoch_accuracy, epoch_val_accuracy, epoch_val_f1)\n",
    "    \n",
    "    # Step 5: Evaluate Model on Test Set\n",
    "    print(\"Evaluating model on test set...\")\n",
    "    test_accuracy, test_f1 = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")\n",
    "    \n",
    "    # Step 6: Save Confusion Matrix for Test Set\n",
    "    print(\"Saving confusion matrix...\")\n",
    "    model.save_confusion_matrix(X_test, y_test, filename=\"confusion_matrix.png\")\n",
    "    \n",
    "    # Step 7: Save the Model\n",
    "    print(\"Saving the model...\")\n",
    "    model.save_model(filename=\"trained_model.pkl\")\n",
    "    \n",
    "    print(\"Process complete!\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
